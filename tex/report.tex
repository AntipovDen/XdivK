\documentclass[russian]{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts}

\usepackage{flushend}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{xspace}
\usepackage[T2A]{fontenc}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage[english,russian]{babel}

\usepackage[utf8]{inputenc}
\usepackage[backend=bibtex]{biblatex}

\usepackage{tabularx}

\newcommand{\OM}{\textsc{OneMax}\xspace}
 \newcommand{\J}{\textsc{Jump}\xspace}
\newcommand{\LB}{\textsc{LeftBridge}\xspace}
\newcommand{\RB}{\textsc{RightBridge}\xspace}
\newcommand{\EARL}{\textsc{EA+RL}\xspace}
\newcommand{\RLS}{\textsc{RLS}\xspace}
\newcommand{\OMZM}{\textsc{OneMax+ZeroMax}\xspace}
\newcommand{\XdK}{\textsc{XdivK}\xspace}
\newcommand{\OPO}{$(1 + 1)$}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\addbibresource{bibliography.bib}
\allowdisplaybreaks

\begin{document}

\section{Введение}

Динамическая настройка параметров является довольно сложным и на данный момент плохо изученным механизмом в области эволюционных вычислений. В некоторых случаях небольшое изменение значений параметров может увеличить ожидаемое время работы алгоритма с полиномиального до экспоненциального~\cite{doerr_mutation_rate_matters,on_one_plus_one}. Однако многие ведущие ученые из области эволюционных вычислений, такие как Эйбен, Хинтердинг и Михалевиц считают, что ислледование стратегий динамической настройки параметров является наиболее перспективной областью развития эволюционных вычислений~\cite{parameter_control}.

В своей недавней работе Б. Доерр и К. Доерр показали, что довольно простое правило одной пятой (\textit{англ. the one-fifth rule}), примененное к размеру популяции в $(1 + \lambda, \lambda)$-эволюционном алгоритме, способно дать асимптотическое ускорение алгоритма, уменьшив ожидаемое время его работы до линейного при решении задачи \OM. В некоторых случаях параметры алгоритма выбираются в каждый момент времени случайным образом исходя из некоторого распределения~\cite{fga}. В данном случае встает вопрос о том, какое распределение стоит выбрать для параметра, чтобы минимизировать время работы алгоритма.

Для оценки эффективности различных стратегий выбора параметров необходимо сравнить их с наиболее эффективными алгоритмами, которые используют фиксированный набор параметров. Задача нахождения наиболее эффективного набора фиксированных параметров также является довольно сложной, так как требует нахождения точного ожидаемого числа итераций алгоритма, а не асимптотического выражения. Также оптимальный набор параметров может отличаться для одного и того же алгоритма на разных ландшафтах оптимизируемой функции.

В данной работе ведется поиск оптимальной силы мутации для \OPO-эволюционной стратегии при прохождении плато фиксированного радиуса. Это исследование является первым шагом к поиску оптимальной стратегии выбора силы мутации для эволюционных алгоритмов.


\section{Постановка задачи}

В данной работе рассматривается \OPO-эволюционная стратегия, которая оптимизирует некоторую функцию $F(x),$ определенную на множестве всех двоичных векторах длины $n$ следующим образом:

\begin{align*}
  F(x) =
  \begin{cases}
    \OM(x), \text{ если } \OM(x) \le n - k \\
    n - k, \text{ если } n - k < \OM(x) < n \\
    n, \text{ если } \OM(x) = n, \\
  \end{cases}
\end{align*}
где $\OM(x)$ --- число единиц в векторе $x.$ То есть целевая функция почти на всем пространстве поиска совпадает с функцией \OM, но имеет плато радиуса $k$ вокруг оптимума.

\OPO-эволюционная стратегия является итеративным алгоритмом оптимизации. В процессе оптимизации она хранит текущее лучшее решение $x.$ На каждом шаге она применяет к $x$ оператор мутации и получает новое решение $y.$ Если новое решение не хуже предыдущего по значению оптимизируемой функции, то алгоритм принимает заменяет $x$ на $y.$ Оператор мутации меняет значение каждого бита на противоположное с вероятностью $\frac{\alpha}{n},$ где $\alpha$ --- это некоторая константа, которую мы называем \textit{силой мутации.} Псевдокод анализируемого алгоритма представлен в Листинге~\ref{pseudo}.

\begin{algorithm}[!t]
\begin{center}
    \begin{algorithmic}[1]
        \State {$x \gets$ случайно сгенерированный двоичный вектор}
        \State {$\textsc{Mutate}(x) \gets$ оператор мутации: изменяет значение каждого бита с вероятностью $\frac{\alpha}{n}$}
        \While {$F(x) < n$}
            \State{$y \gets \textsc{Mutate}(x)$}
            \If {$F(y) \ge F(x)$}
                \State {$x$ $\gets$ $y$}
            \EndIf
        \EndWhile
    \end{algorithmic}
    \caption{Псевдокод \OPO-эволюционной стратегии, оптимизирующей функцию $F(x)$}
    \label{pseudo}
\end{center}
\end{algorithm}

Стоит заметить, что представленный в данной работе анализ будет верным и в том случае, если есть некоторый загаданный двоичный вектор $z$, и $\OM(x)$ равно числу совпадающих бит в векторе $x$ и $z.$ В данном случае рассматриваемый алгоритм отгадывает некоторый неизвестный вектор $z.$ Распостранение анализа на этот случай возможно по той причине, что поведение оператора мутации в рассматриваемом алгоритме не зависит от значений и позиции бит в векторе $x.$

Основной задачей данной работы является нахождение такого значения силы мутации $\alpha,$ при котором ожидаемое число итераций данного алгоритма было бы минимальным.

\section{Нотация и вероятностная модель задачи}

Так как время, за которе алгоритм сможет достичь плато вокруг оптимума не больше, чем время оптимизации этим алгоритмом функции \OM, то оно равно $O(n log n).$ При этом число итераций, необходимых алгоритму на то, чтобы преодолеть плато и добраться до оптимума, есть $\Omega(n^k),$ как будет показано в данной работе. Поэтому временем, которое алгоритм добирается до плато, можно пренебречь и рассматривать только тот отрезок процесса оптимизации, на котором алгоритм проходит через плато.

Для точного анализа времени работы алгоритма, плато может быть представлено с помощью цепи Маркова двумя разными способами. Первая марковская цепь, представляющая плато состоит из $N = \sum\limits_{i = 0}^{k - 1} \binom{n}{k - 1}$ состояний, и каждое состояние соответствует одному из двоичных векторов, принадлежащих плато. Вероятность перехода из одного состояния, соответствующего вектору $x,$ в другое состояние, соответствующее вектору $y,$ равна вероятности того, что $\textsc{Mutate}(x) = y,$ и зависит только от расстояния Хэмминига между этими векторами. Также в данную цепь входит и еще одно состояние, соответствующее оптимуму функции $F.$

Матрица переходов данной цепи $P$ имеет большой размер, и ее довольно сложно описать формально. Однако если исключить из цепи состояние, соответствующее оптимуму (при этом не меняя вероятностей переходов), то матрица $P$ будет симметричной: чтобы мутировать из состояния $x$ в состояние $y,$ надо изменить те же самые биты, что и для обратной мутации. Это дает нам право в ходе анализа пользоваться свойствами симметричных матриц, например тем, что все собственные вектора симметричных матриц ортогональны. Мы будем называть эту цепь
\textit{цепью индивидов,} а все вещественные вектора длины $N$ мы будем называть \textit{пространством индивидов.} Введение пространства индивидов необходимо, потому что все распределения вероятностей по состояниям цепи индивидов принадлежат этому пространству.

Второй способ представления плато --- это марковская цепь, состоящая из $k$ состояний, причем состояние с номером $i$ соответствует всем двоичным векторам длины $n,$ содержащим ровно $n - k + i$ нулей и называется \textit{$i$-ым уровнем.} Данная цепь также содержит одно состояние, соответствующего оптимуму. Матрица перехода $P_\ell$ данной матрицы несложно описать формально, однако она не является симметричной, как матрица $P.$ Мы будем называть эту цепь \textit{цепью уровней,} а все вещественные вектора длины $k$ --- \textit{пространством уровней} по аналогии с пространством индивидов.

Существует отображение $\phi$ из пространства уровней в пространство индивидов, возникающее естественным образом. Каждому вектору $x = (x_0, \dots, x_{k - 1})$ можно поставить в соответствие вектор $y = \phi(x) = (y_0, \dots, y_{N - 1}),$ где $y_i = x_j / \binom{n}{k - j}, $ если $i$-й индивид принадлежит к уровню $j$. Это отображение стоит понимать следующим образом. Если $x$ это распределение некоторой массы по уровням плато, то это распределение должно быть равномерным внутри каждого уровня, так как индивиды внутри каждого уровня симметричны относительно переиндексации бит. Отображение $\phi$ обладает некоторыми хорошими свойствами:

\begin{enumerate}
  \item Линейность: $\phi(a x_1 + b x_2) = a \phi(x_1) + b \phi(x_2),$ где $a, b \in \mathbb{R}.$ Это свойство следует прямо из определения $\phi.$
  \item $\phi(x P_\ell) = \phi(x) P.$ Это следует из двух фактов. Во-первых, если некоторый вектор $y$ из пространства индивидов обладает тем свойством, что внутри каждого уровня плато значение его компонент одинаково, то после применения к вектору $y$ матрицы $P$ это свойство останется верным. Данный факт объясняется симметричностью пространства индивидов относительно переиндексации номеров бит в каждом индивиде. Таким образом, если вектор $y$ из пространства индивидов имеет некоторый соответствующий ему вектор $x_1$ из пространства уровней, вектор $yP$ тоже будет иметь некоторыйсоответствующий ему вектор $x_2$ из пространста уровней. Во вторых, переход массы между уровнями в матрице $P$ точно такой же, как в матрице $P_\ell.$ Следовательно, $x_2 = x_1 P.$
  \item Из предыдущих свойств следует, что если $x$ является собственным вектором $P_\ell,$ то $\phi(x)$ --- собственный вектор матрицы $P.$ Отсюда также следует, что спектр матрицы $P_\ell$ содержит в себе спектр матрицы $P:$ $\sigma(P) \subset \sigma(P_\ell).$
  \item Если мы используем манхэттенскую метрику в обоих пространствах, то $|x| = |\phi(x)|,$ так как компоненты $\phi(x)$ с одного уровня имеют один и тот же знак. Заметим также, что для евклидовой метрики данное свойство не выполняется.
\end{enumerate}

В данной работе мы используем как манхэттенскую, так и евклидову метрики. Поэтому модуль вектора $x$ по манхэттенской метрике мы обозначаем как $|x| = \sum\limits_{i} |x_i|.$ Модуль вектора $x$ по евклидовой метрике --- $|x|_e = \sqrt{\sum\limits_i x_i^2}.$

\section{Спектр матрицы переходов}

Рассмотрим матрицу переходов для цепи уровней $P_\ell.$ Ее элементы $p_i^j$ могут быть записаны следующим образом:

\begin{align*}
  p_i^j = \begin{cases}
    0, \text{ if } i = k, j \ne k, \\
    1, \text{ if } i = j = k, \\
    \sum\limits_{m = 0}^{k - j} \binom{k - i}{j - i + m} \binom{n - k + i}{m} \left(\frac{\alpha}{n}\right)^{j - i + 2m} \left(1 - \frac{\alpha}{n}\right)^{n - j + i - 2m}, \text{ if } j > i, \\
      \sum\limits_{m = 0}^{k - i} \binom{k - i}{m} \binom{n - k + i}{i - j + m} \left(\frac{\alpha}{n}\right)^{i - j + 2m} \left(1 - \frac{\alpha}{n}\right)^{n - i + j - 2m}, \text{ if } j < i \text{ and } i \ne k, \\
      1 - \sum\limits_{m = 0, m \ne i}^k p_i^m, \text{ if } j = i \ne k. \\
  \end{cases}
\end{align*}

Рассмотрим только ту часть этой матрицы, которая не содержит состояние, соответствующее оптимуму. То есть мы рассматриваем только $p_i^j,$ где $i, j \in [0..k-1].$

Заметим, какой порядок имеют элементы матрицы. Если $j < i,$ тогда $p_i^j$ это некоторая константа, которая примерно равна $\binom{n - k + i}{i - j}\left(\frac{\alpha}{n}\right)^{i - j}e^{-\alpha} \approx \frac{\alpha^{i - j}}{(i - j)!} e^{-\alpha}.$ Если $j > i,$ то значение $p_i^j$ очень, его можно примерно оценить как $\binom{k - i}{j - i}\left(\frac{\alpha}{n}\right)^{j - i}e^{-\alpha} = \Theta(n^{-(j - i)}).$

Самое важное --- это порядок $p_i^i.$ Так как $p_i^i = 1 - \sum\limits_{j \ne i} p_i^j$ И нам известно примерное значение $p_i^j$, то $p_i^i = 1 - \sum\limits_{j = 1}^{i} \frac{\alpha^j}{j!}e^{-\alpha} + o(1).$  Это некоторая константа, которая меньше, чем $1$ (эта граница почти достигается при $i = 0$) но точно больше, чем $e^{-\alpha}$ (эта граница почти достигается при $i = k - 1$ при больших значениях $k$).

Заметим два предварительных факта про спектр матрицы $\P_\ell:$
\begin{enumerate}
  \item Так как спектр матрицы $P_\ell$ является подмножеством спектра симметричной матрицы $P,$ то все ее собственные числа вещественны.
  \item По теореме Перрона-Фробениуса, так как матрица $P_\ell$ является матрицей с положительными элементами, то самое большое по модулю собственное число этой матрицы не превосходит максимальную сумму строки матрицы, то есть оно строго меньше единицы.
\end{enumerate}

Теперь чтобы найти спектр матрицы $P_\ell,$ нам надо найти все корни ее характеристического полинома $\chi(\lambda),$ который является детерминантом матрицы $(P - \lambda I):$
\begin{align*}
  \chi(\lambda) = \det (P - \lambda I) = \sum\limits_{\sigma \in S_k} \text{sgn}(\sigma) \prod\limits_{i = 0}^{k - 1}(P - \lambda I)_{i, \sigma{i}},
\end{align*}
где $S_k$ это множество всех перестановок множества $[0..k - 1].$ Заметим, что для всех перестановок, кроме $\sigma_0 = (0, 1, \dots, k - 1)$ произведение под знаком суммы имеет хотя бы один множитель $(P - \lambda I)_{i, j},$ в котором $j > i$. Но этот множитель $(P - \lambda I)_{i, j} = p_i^j = o(1)$. Остальные элементы произведения или $p_i^j,$ или $(p_i^i - \lambda),$ но в обоих случаях их модуль ограничен сверху единицей. Таким образом, мы можем записать характеристический полином следующим образом:

\begin{align*}
  \chi(\lambda) = \prod\limits_{i = 0}^{k - 1} (p_i^i - \lambda) + o(1).
\end{align*}

Так как все коэффициенты полинома почти константы по $n$, то мы можем рассматривать график данного полинома как небольшой сдвиг графика полинома без $o(1)$ по вдоль вертикальной оси. Значит, корни характеристического полинома находятся в некоторой окрестности корней полинома $\prod\limits_{i = 0}^{k - 1} (p_i^i - \lambda).$ Причем радиус этих окрестностей стремится к нулю при $n$ стремящимся к бесконечности. Более того, все корни вещественны, как упоминалось в первом факте о спектре матрицы $P_\ell.$

Поэтому мы можем сказать, что спектр матрицы $P_\ell$ есть $\{p_i^i \pm o(1)\}_{i = 0}^{k - 1}.$ В дальнейшем анализе нам будут важны следующие свойства данного спектра:
\begin{itemize}
\item Все собственные числа положительны.
\item Минимальный элемент спектра $\lambda_{k - 1}$ не меньше, чем $1/e - o(1).$
\item Самый большой элемент спектра $\lambda_0 = 1 - o(1).$ Как уже упоминалось, по теореме Перрона-Фробениуса можно также сказать, что $\lambda_0$ строго меньше единицы (то есть $o(1)$ отрицательно).
\item Второй по величине элемент спектра $\lambda_1$ равен $p_1^1 (1 + o(1)) = 1 - 1/e + o(1).$
\end{itemize}

\section{Анализ времени работы}

Из теоремы Перрона-Фробениуса~\cite{} следует, что самое большое собственное значение имеет одномерное собственное подпространство, а соответствующий ему собственный вектор имеет только положительные (или только отрицательные) компоненты. Пусть $\pi^*$ это некоторый стохастический вектор длины $k$, соответствующий распределению вероятностей по состояниям цепи уровней, причем такому, что через одну итерацию каждая компонента этого распределения уменьшится в некоторую константу раз, равную для всех компонент. Назовем такое распределение \textit{условно стационарным}. Заметим, что $\pi^*$ является собственным вектором матрицы $P_\ell$, а так как она имеет единственный собственный вектор со всеми положительными компонентами, то соответствующее ему собственное число есть $\lambda_0.$

Также введем стохастический вектор $u$ который соответствует распределению в пространстве уровней, которое является равномерным в пространстве индивидов. То есть каждый уровень имеет вероятностную массу $u_i:$

\begin{align*}
  u_i = \binom{n}{k - i} / \sum\limits_{j = 0}^{k - 1} \binom{n}{k - j} = \binom{n}{k - i}N^{-1}.
\end{align*}

Пусть $e^i$ это $i$-й собственный вектор матрицы $P_\ell$ который соответствует $i$-му по величине собственному числу $\lambda_i.$ Нормируем $e^0$ так, чтобы $|e^0| = 1.$ Таким образом $e^0 = \pi^*.$ В случаях, когда мы будем рассматривать этот вектор как собственный вектор матрицы, мы будем использовать обозначение $e^0,$ когда же данный вектоор будет рассматриваться как распределение, то будет использоваться обозначение $\pi^*.$

Множество векторов $\{e^i\}_{i = 0}^{k - 1}$ образует базис пространства $\mathbb{R}^k.$  Это значит, что существует единственный набор коэффициентов $(c_0, c_1, \dots, c_{k - 1})$ такой что $u = \sum\limits_{i = 0}^{k - 1} c_i e^i.$

\begin{lemma}\label{lemma_uniform}
$\pi^*$ почти является равномерным распределением: для каждого $j \in [0..k-1]$ верно, что $\pi^* = u_j (1 + O(1/\sqrt{n})).$
\end{lemma}

\begin{proof}

Введем обозначения: $U$ будет означать вектор равномерного распределения в пространстве индивидов, то есть равен $\phi(u),$ и $U^i$ будет означать вектор из пространства индивидов, соответствующий $i$-ой компоненте вектора $u$ в базисе из $e^i$, который равен $c_i\phi(e^i).$ Заметим, что $U^i$ являются собственными векторами матрицы $P.$

По линейности отображения $\phi$ можно сказать, что $U = \sum\limits_{i = 0}^{k - 1} U^i,$ что $U^i$ это компоненты вектора $U$ в базисе из собственных векторов симметричной матрицы $P,$ и следовательно, они ортогональны друг другу.

Для каждого $j \in [0..N-1]$ верно следующее:

\begin{align*}
  U_j = \sum\limits_{i = 0}^{k - 1} U^i.
\end{align*}

Вспомним, что $U_j = \frac{1}{N}$ для всех $j.$ Поэтому мы можем найти $j$-ую компоненту вектор $U^0:$

\begin{align*}
  U_j^0 = U_j - \sum\limits_{i = 1}^{k - 1} U_j^i = U_j \left(1 - \frac{\sum\limits_{i = 1}^{k - 1} U_j^i}{U_j}\right) = U_j \left(1 - N\sum\limits_{i = 1}^{k - 1} U_j^i\right)
\end{align*}

Таким образом, для доказательства леммы нам требуется доказать, что для каждого $j$ верно, что $N\sum\limits_{i = 1}^{k - 1} U_j^i = O(1/\sqrt{n}).$

Для этого рассмотрим вектор $U - UP.$ С одной стороны, компоненты этого вектора очень малы:

\begin{align*}
  (U - UP)_j = U_j - \sum\limits_{i = 0}^{N - 1} P_i^j U_i = \frac{1}{N} \left( 1 - \sum\limits_{i = 0}^{N - 1} P_j^i\right) = \frac{P_j^N}{N},
\end{align*}
где $P_j^N$ это вероятность покинуть плато, находясь в $j$-ом индивиде. Поэтому евклидова мера данного вектора также мала:

\begin{align*}
|U - UP|_e &= \sqrt{\sum\limits_{j = 0}^{N - 1} \left(\frac{P_j^N}{N}\right)^2} = \frac{1}{N} \sqrt{\sum\limits_{i = 0}^{k - 1} \binom{n}{k - i} \left(\frac{\alpha}{n}\right)^{2(k - i)} e^{-2\alpha} (1 + o(1))} \\
&= \frac{1}{N} \sqrt{\frac{\alpha^2}{ne^2} + o(1/n)} = \frac{\alpha}{\sqrt{n}eN} (1 + o(1)).
\end{align*}

С другой стороны, если вспомнить, что все $U^i$ являются собственными векторами, мы можем записать следующее равенство:

\begin{align*}
  U - UP = \sum\limits_{i = 0}^{k - 1} U^i - \sum\limits_{i = 0}^{k - 1} \lambda_i U^i = \sum\limits_{i = 0}^{k - 1} (1 - \lambda_i) U^i
\end{align*}

Так как $U^i$ ортогональны, то для каждого $i \in [0..k - 1]$ можем сказать, что

\begin{align*}
  |(1 - \lambda_i) U^i|_e \le |U - UP_\ell|_e \approx \frac{\alpha}{\sqrt{n} e N}.
\end{align*}

А абсолютное значение каждой компоненты вектора не может быть больше, чем евклидова мера данного вектора:

\begin{align*}
  |(1 - \lambda_i) U_j^i| \le |(1 - \lambda_i) U^i|_e \le \frac{\alpha}{\sqrt{n} e N} (1 + o(1)).
\end{align*}

Для каждого $j$ можно записать следующую оценку:

\begin{align*}
  |N\sum\limits_{i = 1}^{k - 1} U_j^i| &\le N\sum\limits_{i = 1}^{k - 1} |U_j^i| \le N\sum\limits_{i = 1}^{k - 1} \frac{\alpha}{\sqrt{n} e N (1 - \lambda_i)} (1 + o(1)) \\
  & \le \frac{\alpha (k - 1)}{\sqrt{n}e (1 - \lambda_1)} = O\left(\frac{1}{\sqrt{n}}\right).
\end{align*}

Наконец,

\begin{align*}
  \phi(\pi^*) = \frac{U^0}{|U^0|} = \frac{U^0}{(1 + O(1/\sqrt{n}))} = U^0 (1 + O(1/\sqrt{n})),
\end{align*}
поэтому для любого $j$ верно:
\begin{align*}
  \pi_j^* = c_0 e_j^0 (1 + O(1/\sqrt{n})) = u_j (1 + O(1/\sqrt{n})).
\end{align*}


\end{proof}

Данную лемму можно понимать следующим образом: так как вектор $u$ почти не меняется под действием матрицы $P_\ell$ и так как самое большое собственное значение этой матрицы $\lambda_0 = (1 - o(1)$ а остальные собственные значения отделены от него, то $u$ почти не отличается от собственного вектора, соответствующего $\lambda_0$.

\begin{theorem}
  Ожидаемое время прохождения \OPO-эволюционной стратегией плато вокруг оптимума радиуса $k$ есть $N \left( e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!}\right)^{-1} (1 + o(1)).$
\end{theorem}

\begin{proof}
  Для доказательства этой теоремы мы используем формулу матожидания случайной величины $X,$ принимающей только целые неотирцательные значения: $E[X] = \sum\limits_{t = 1}^{+\infty} \Pr[X \ge t].$

  Если есть некоторое начальное распределение по уровням плато $\pi,$ то вероятность, что время работы не меньше $t$ есть вероятность остаться на плато после итерации $t$, которая равна $|\pi P^t|.$ Это дает нам следующее выражение для ожидаемого времени работы:

\begin{align*}
  E[T] = \sum\limits_{t = 0}^{+\infty} |\pi P^t|.
\end{align*}

Теперь надо оценить $|\pi P^t|.$ Для этого разложим $\pi$ по базису из собственных векторов $P_\ell:$

\begin{align*}
  \pi = \sum\limits_{i = 0}^{k - 1} \pi^i.
\end{align*}

Верхнюю оценку можно получить следующим образом:

\begin{align*}
  E[T] = \sum\limits_{t = 0}^{+\infty}|\pi P^t| \le \sum\limits_{t = 0}^{+\infty} \sum\limits_{i = 0}^{k - 1} \lambda_i^t |\pi^i| = \sum\limits_{i = 0}^{k - 1} |\pi^i| \sum\limits_{t = 0}^{+\infty} \lambda_i^t = \sum\limits_{i = 0}^{k - 1} \frac{|\pi^i|}{1 - \lambda_i}.
\end{align*}

Нижнюю оценку можно получить аналогично:
\begin{align*}
  E[T] = \sum\limits_{t = 0}^{+\infty}|\pi P^t| \ge \sum\limits_{t = 0}^{+\infty} \left(\lambda_0^t |\pi^0| - \sum\limits_{i = 1}^{k - 1} \lambda_i^t |\pi^i|\right) = \frac{|\pi^0|}{1 - \lambda_0} - \sum\limits_{i = 0}^{k - 1} \frac{|\pi^0|}{1 - \lambda_i}.
\end{align*}


Для $i \ne 0$ $\frac{1}{1 - \lambda_i} \le e.$ Также несложно дать грубую оценку на все $|\pi_i|:$

\begin{align*}
|\pi^i| = |\phi(\pi^i)| \le \sqrt{N}|\phi(\pi^i)|_e \le \sqrt{N}|\phi(\pi)|_e \le \sqrt{N}|\phi(\pi)| = \sqrt{N}.
\end{align*}

Это грубая оценка, но ее хватает для нашего анализа. Она дает нам следующий результат:

\begin{align*}
  \sum\limits_{i = 1}^{k - 1} \frac{|\pi^i|}{1 - \lambda_i} \le (k - 1)e\sqrt{N} = O(\sqrt{N}).
\end{align*}

далее нам требуется оценить $|\pi^0|$ и $\lambda_0$.

Сначала, чтобы найти $|\pi^0|,$ заметим, что $\pi^0 = |\pi^0|e^0.$ Напомним, что ранее мы ссылались на этот вектор как на $\pi^*,$ так как рассматривали его в качестве распределения вероятностей по состояниям марковской цепи, однако сейчас мы будем использовать обозначение $e^0,$ чтобы подчеркнуть, что $\phi(e^0)$ является одним из собственных векторов, формирующих ортогональный базис пространства индивидов. Из Леммы~\ref{lemma_uniform} известно, что все компоненты $\phi(e^0)$ почти равны соответствующим компонентам равномерного распределения: $\phi(e^0)_j = \frac{1 + O(1/\sqrt{n})}{N}.$

Так как $\phi(\pi^0)$ является проекцией $\phi(\pi)$ на собственное подпространство $\lambda_0$  в пространстве индивидов, то можно рассматривать $|\pi^0|$ как координату $\phi(\pi)$ в базисе из собственных векторов, которая соответствует базисному вектору $\phi(e^0).$ Поэтому можно оценить ее значение следующим образом:

\begin{align*}
  |\pi^0| = |\phi(\pi^0)| = \frac{<\phi(\pi^0), \phi(e^0)>}{<\phi(e^0), \phi(e^0)>} = \frac{\sum\limits_{j = 0}^{N - 1} \pi_j^0 \frac{1 + O(1/\sqrt{n})}{N}}{\sum\limits_{j = 0}^{N - 1} \left( \frac{1 + O(1/\sqrt{n})}{N}\right)^2} = 1 + O(1/\sqrt(n)).
\end{align*}

Для нахождения $\lambda_0$ заметим, что $(1 - \lambda_0)$ это вероятностная масса, которую теряет вектор $\pi^*$ после применения к нему матрицы $P_\ell.$ Это можно обосновать следующим образом:

\begin{align*}
  1 - \lambda_0 &= |(1 - \lambda_0)\pi^*| = |\pi^* - \pi^* P| = \sum\limits_{i = 0}^{k - 1} |\pi_i^* - \sum\limits_{j = 0}^{k - 1} \pi_j^* p_j^i| \\
  &= \sum\limits_{i = 0}^{k - 1} (\pi_i^* - \sum\limits_{j = 0}^{k - 1} \pi_j^* p_j^i) = 1 - \sum\limits_{i = 0}^{k - 1} \sum\limits_{j = 0}^{k - 1} \pi_j^* p_j^i \\
  &= 1 - \sum\limits_{j = 0}^{k - 1} \pi_j^* \sum\limits_{i = 0}^{k - 1} p_j^i = 1 - \sum\limits_{j = 0}^{k - 1} \pi_j^* (1 - p_j^k) = \sum\limits_{j = 0}^{k - 1} \pi_j^* p_j^k.
\end{align*}

Потерянную массу можно вычислить:

\begin{align*}
  1 - \lambda_0 &= \sum\limits_{i = 0}^{k - 1} \pi_i^* p_i^k = \sum\limits_{i = 0}^{k - 1} \binom{n}{k - i}N^{-1}(1 + O(1/\sqrt{n})) \frac{\alpha^{k - i}}{n^{k - i}}e^{-\alpha} (1 + o(1)) \\
  &= \frac{1}{N} \sum\limits_{i = 1}^{k} \frac{\alpha^i}{i!}e^{-\alpha} (1 + o(1)).
\end{align*}

Наконец, мы имеем следующую верхнюю оценку на время работы:

\begin{align*}
  E[T] \le \frac{|\pi^0|}{1 - \lambda_0} + O(\sqrt{N}) = \frac{N}{e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!}} (1 + o(1))
\end{align*}
и такую же нижнюю оценку:
\begin{align*}
  E[T] \ge \frac{|\pi^0|}{1 - \lambda_0} - O(\sqrt{N}) = \frac{N}{e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!}} (1 + o(1)).
\end{align*}

\end{proof}

\section{Оптимальное значение силы мутации}

Оптимальное значение $\alpha$ минимизирует $E[T]$ и, следовательно, максимизирует знаменатель в выражении для $E[T]:$

$$ e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!}$$

Данное выражение является непрерывной гладкой функцией по $\alpha.$ Для нахождения его максимального значения требуется рассмотреть его значение на концах допустимого отрезка значений, а также в нулях его производной. При $\alpha = 0$ и при $\alpha \to +\infty$ данное выражение равно (стремится) к нулю. Поэтому найдем его производную:

\begin{align*}
(e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!})'
       &= e^{-\alpha} \sum\limits_{i = 1}^k \frac{i\alpha^{i - 1}}{i!} - e^{-\alpha} \sum\limits_{i = 1}^k \frac{\alpha^i}{i!} \\
       &= e^{-\alpha}\left( \sum\limits_{i = 0}^{k - 1} \frac{\alpha^i}{i!} - \sum\limits_{i = 1}^k \frac{\alpha^i}{i!} \right) = e^{-\alpha} (1 - \frac{\alpha^k}{k!})
\end{align*}

$(p_a)' = 0$ только при $\alpha = \sqrt[k]{k!}.$

Значение знаменателя $E[T]$ при таком значении $\alpha$ будет некоторой константой строго больше нуля, следовательно данное значение $\alpha$ является асимптотически оптимальным.

% \section{Следствия}
%
% Из приведенного анализа можно вывести несколько интересных следствий. Данный анализ можно расширить и на некоторые другие эволюционные алгоритмы, которые оптимизируют такую же функцию, сравнив их эффективность с эффективностью \OPO при оптимальной силой мутации.
%
% \subsection{Сравнение с несмещенным оператором мутации}
%
% Несмещенные операторы мутации в общем случае выглядят следующим образом: сначала они выбирают из некоторого распределения число бит, которые будут мутированы, а затем изменяют значения выбранного числа случайных бит. Я начал писать этот раздел, а потом вдруг понял, что у меня были полные результаты только для ошибочного решения (то есть еще в декабре). Тут должно получиться что-то вроде того, что для любого распределения, из которого выбирается число бит для мутации, время работы будет одинаково при двух условиях:
%
% \begin{enumerate}
%   \item Вероятность мутировать ровно один бит больше некоторой константы.
%   \item Вероятность мутировать более чем $k$ бит есть $o(1).$
% \end{enumerate}
%
% Но это все не очень точно.
%
% \subsection{Сравнение с Fast Genetic Algorithm}

\printbibliography

\end{document}
