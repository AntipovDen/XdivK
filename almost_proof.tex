\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{flushend}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{xspace}
\usepackage[T2A]{fontenc}

\usepackage{algorithm}
\usepackage[english]{babel}

\usepackage[utf8]{inputenc}
\usepackage[backend=bibtex]{biblatex}

\newcommand{\OM}{\textsc{OneMax}\xspace}
 \newcommand{\J}{\textsc{Jump}\xspace}
\newcommand{\LB}{\textsc{LeftBridge}\xspace}
\newcommand{\RB}{\textsc{RightBridge}\xspace}
\newcommand{\EARL}{\textsc{EA+RL}\xspace}
\newcommand{\RLS}{\textsc{RLS}\xspace}
\newcommand{\OMZM}{\textsc{OneMax+ZeroMax}\xspace}
\newcommand{\XdK}{\textsc{XdivK}\xspace}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\addbibresource{bibliography.bib}
\allowdisplaybreaks

\begin{document}

In this document I have summed up all the current knowledge about the optimal value of the mutation rate for the $(1 + 1)$-ES optimizing a function with a plateau around its global optima of radius $k$. This paper may be considered as a sceleton of the final proof, as I belive that I can not finish it only because of lack of knowledge and experience in some areas of linear algebra. First three sections are identical to the first three sections of the previous paper ("current progress"), but with fixed bugs in math and in my English.

\section{Problem statement}

We consider simple $(1 + 1)$-ES. It has only one individual in population and on each iteration it creates a new individual by mutating each bit of the current individual with probability of $\frac{\lambda}{n}$, where $\lambda$ is a mutation rate that is a fixed constant during an algorithm run. It accepts the new individual if and only if new individual's fitness value is not worse then the fitness value of the curent individual.

At first we considered \XdK function. But it seemed too complicated, so we simplified it to the following function $F$ that has some parameter $k$, as \XdK does:

\begin{align*}
  F(x) =
  \begin{cases}
    \OM(x), \text{ if } \OM(x) \le n - k \\
    n - k, \text{ if } n - k < \OM(x) < n \\
    n, \text{ if } \OM(x) = n \\
  \end{cases}
\end{align*}

Our aim is to find the optimal value of the mutation rate $\lambda$ that minimizes the expected runtime of the algorithm. If we reach our goal, we will be able to compare the considered algorithm with the Fast Genetic Algorithm.

\section{First subproblem}
As passing over the plateau in the end of the optimization process requires most time (at least $\Omega(n^2)$ for $k \ge 2$, while reaching the plateau requires only $O(n \log n)$), we are most interested in finding the optimal value of $\lambda$ for passing the plateau. So the first subproblem (that is actually our source problem) is to find a dependence of exact expected runtime of passing the plateau on the mutation rate.

\section{Notation}

We consider a markov chain that illustrates the behaviour of the considered algorithm on the plateau of the function $F$. It has $k + 1$ states that are numbered from $0$ to $k$. If the algorithm is in state $i$, then the current individual has exactly $n - k + i$ one-bits in it. State $k$ is a final state, so we aim to find the expected number of iterations needed to get there.

By $p_i^j$ we imply the probability that algorithm being in state $i$ will get to state $j$ in one iteration.

The exact value of this probabilities is quite complicated:

\begin{align*}
  p_i^j = \begin{cases}
    \sum\limits_{m = 0}^{k - j} \binom{k - i}{j - i + m} \binom{n - k + i}{m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m}, \text{ if } j > i \\
      \sum\limits_{m = 0}^{k - i} \binom{k - i}{m} \binom{n - k + i}{i - j + m} \left(\frac{\lambda}{n}\right)^{i - j + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - i + j - 2m}, \text{ if } j < i \\
      1 - \sum\limits_{m = 0, m \ne i}^{k + 1} p_i^m, \text{ if } j = i \\
  \end{cases}
\end{align*}

Let us also notice that the first summand in the sum for $i \ne j$ is the most significant one: every next summand is less then previos one by factor at least $\frac{n}{k\lambda^2}$, so in some equations of this work we consider $p_i^j$ as its first summand multiplied by factor $(1 + o(1)).$

By $\pi_i(t)$ we imply the probability that algorithm will be in state $i$ on the $t$-th iteration with condition that algorithm does not reach the final state before $t$-th iteration. Also we define $\pi(t)$ as a stochastic vector $(\pi_0(t), \pi_2(t), \dots, \pi_{k - 1}(t)).$ Notice that $\pi(0) = (1, 0, \cdots, 0),$ although it does not affect the proof.

By $p_{end}(t)$ we imply the probability that algorithm will reach the final state on the $t$-th iteration with condition that it does not reach it before. Notice that:

$$p_{end}(t) = \sum\limits_{i = 0}^k \pi_i(t) p_i^k$$

\section{Plan of the proof}

First we will find a stationary distribution over the states of the markov chain, excluding state $k$. It is a distribution $\pi^*$ such that

$$\pi(t) = \pi^* \Rightarrow \pi(t + 1) = \pi^*.$$

Also we will find $p_{end}^*$ that is a probability to reach the final state for stationary distribution (just like $p_{end}(t)$ for $\pi(t)$).

Next step will be showing that $\pi(t)$ becomes very close to $\pi^*$ in just a few iterations. Then we will show that the total expected runtime is actually $\frac{1}{p_{end}^*} +o\left(\frac{1}{p_{end}^*} \right)$. The last step is finding the value of mutation rate $\lambda$ that minimizes $\frac{1}{p_{end}^*}.$

\section{Stationary distribution}

Obviously, algorithm will almost surely reach the final state. So the stationary distribution over all the states of markov chain will be $(0, \dots, 0, 1)$ and it does not help us to find the expected runtime. But we actualy are interested, what will be the distribution over the states of markov chain on the $t$-th iteration, if algorithm does not reach final state before this iteration.

From now we will consider slightly another markov chain. This chain has the same states as the original one, except the final state ($k$ states in total). It also has the same transition probabilities, but all the transition probabilities from state $i$ are normalized by factor $\frac{1}{1 - p_i^k},$ as we exclude the probability to go to the final state $k$. Transition matrix for this chain is $P = \left(\frac{p_j^i}{1 - p_j^k}\right),$ where $i \in [0; k - 1]$ is a line index and $j \in [0;k - 1]$ is a column index. If we know $\pi(t)$ then we can find $\pi(t + 1) = \pi(t) \cdot P.$

In this section we aim to find a stationary distribution over new markov chain $\pi^*$ such that $\pi^* = \pi^* \cdot P.$


\begin{lemma}[Stationary distribution]
$\forall i \in [0; k - 1] \pi_i^* = C (1 - p_i^k) \binom{n}{k - i},$ where $C = \left(\sum\limits_{i = 0}^{k - 1} (1 - p_i^k) \binom{n}{k - i}\right)^{-1}$ is a normalization constant.
\end{lemma}

To prove this lemma we first need to prove an additional statement.

\begin{lemma}[There and back again]
$p_i^j \binom{n}{k - i} = p_j^i \binom{n}{k - j}$
\end{lemma}

\begin{proof}
For the case when $i = j$ this lemma is obvious.

Consider the case when $j > i.$ Then:

\begin{align*}
  p_i^j \binom{n}{k - i} &= \sum\limits_{m = 0}^{k - j} \binom{n}{k - i} \binom{k - i}{j - i + m} \binom{n - k + i}{m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m} \\
  &= \sum\limits_{m = 0}^{k - j} \binom{n}{k - j} \binom{k - j}{m} \binom{n - k + j}{j - i + m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m} \\
  &= p_j^i \binom{n}{k - j}.
\end{align*}

The case when $j < i$ can be proven by switching $i$ and $j$ in the previous case.
\end{proof}

\begin{proof}[Proof (Stationary distribution)]
Let $\pi$ be $\left(C (1 - p_i^k) \binom{n}{k - i}\right)_{i = 0}^{k - 1}$. We need to proof that $\pi = \pi \cdot P.$

For some integer $i \in [0, k - 1]$ consider the $i$-th element of $\pi \cdot P$:
\begin{align*}
(\pi \cdot P)_i &= \sum\limits_{j = 0}^{k - 1} \pi_j \frac{p_j^i}{1 - p_j^k} = \sum\limits_{j = 0}^{k - 1} C \binom{n}{k - j} p_j^i \\
&= \sum\limits_{j = 0}^{k - 1} C \binom{n}{k - i} p_i^j = C (1 - p_i^k) \binom{n}{k - i} \sum\limits_{j = 0}^{k - 1} \frac{p_i^j}{1 - p_i^k} \\
&= C (1 - p_i^k) \binom{n}{k - i} = \pi_i.
\end{align*}

It means that $\pi$ is a stable distribution for the markov chain, and as all the sates of the considered markov chain are irreducible and positive reccurent (\textit{is it obvious?}), then such distribution is unique, so $\pi^* = \pi$.
\end{proof}

\section{Convergence to the stable distribution}

This section has some spaces in it, but I do not think it will be hard to fill them.

As our chain is positive reccurent, irreducible and aperiodic (\textit{is it also obvious?}), then all $\pi_i(t) \to \pi^*_i$ if $t \to +\infty.$ The question we'd like to answer is how fast does it converge. Experiments has shown that for every $i$ the difference between $\pi_i(t)$ and $\pi^*_i$ absolutely decreases like a geometric progression with ration about $0.65$ for every $i.$ However ratios for different $i$ are slightly different.

But actually we are interested in the speed of convergence of $p_{end}(t)$ to $p_{end}^*$. Let's consider their difference:

\begin{align*}
  |p_{end}^* - p_{end}(t)| &= |\sum\limits_{i = 0}^{k - 1} p_i^k \pi^*_i - \sum\limits_{i = 0}^{k - 1} p_i^k \pi_i(t)| = |\sum\limits_{i = 0}^{k - 1} p_i^k (\pi^*_i - \pi_i(t))| \\
  &\le \sum\limits_{i = 0}^{k - 1} (p_i^j)^2 \cdot |\pi^* - \pi(t)| \le |\pi^* - \pi(t)|.
\end{align*}

In the last inequation we used HÃ¶lder's inequality (for euclidean measure).

Now we will bound $|\pi^* - \pi(t)|$ considering our process as a power iteration method~\cite{pow-it}. Let $e_1, e_2, \dots, e_k$ be left eigenvectors of the matrix $P$ and $|\lambda_1| \ge |\lambda_2| \ge \dots \ge |\lambda_k|$ be corresponding eigenvalues. As $P$ is a stochastic matrix, then $\lambda_1 = 1.$ $P$'s eigenvectors form a basis, so there are some constants $c_1, c_2, \dots, c_k$ such that $\pi(0) = \sum\limits_{i = 0}^k c_i e_i.$
Then $\pi(1) = \sum\limits_{i = 1}^k \lambda_i c_i e_i.$ After $t$ iterations $\pi(t) = \sum\limits_{i = 1}^k \lambda_i^t c_i e_i.$ Recall that $\lambda_1 = 1$ and $\pi^* = c_1 e_1$ then:

\begin{align*}
  |\pi^* - \pi(t)| &= |c_1 e_1 - \sum\limits_{i = 1}^k \lambda_i^t c_i e_i| = |\sum\limits_{i = 2}^k \lambda_i^t c_i e_i | \\
  &\le |\lambda_2|^t \sum\limits_{i = 2}^k |c_i e_i| = |\lambda_2|^t C_0,
\end{align*}

where $C_0$ is some constant. \textit{One of the spaces in this section is that I do not know how to prove that $C_0$ does not depent on $n$ (or at least it remains a constant for $n \to +\infty$).}

Now we can say that $|p_{end}^* - p_{end}(t)| \le C_0 |\lambda_2|^t$

Next space is proving that $\lambda_2 < const(n) < 1.$ It is the next space in this section. First, I thougt that this fact may be proven using Theorem 1 from~\cite{secondev}. But unfortunately, as I found empirically, the value of the constant that may be obtained with that theorem becomes very close to value of 1 with growing $k$, so it is not the best way. As experiments has shown, the value of $\lambda_2$ does not depend on $n$ but slightly depend on $k$ and is always less then $0.7$.

If we fill the spaces, then we will be able to prove that  $|p_{end}^* - p_{end}(t)|$ is geometric progression (or at least is bounded above by one). Despite of lack of any theoretical proof, I use this fact in the next section.

\section{Runtime proof}.

\begin{theorem}\label{runtime_th}
  The expected runtime of $(1 + 1)$-ES on the function $F$ is $1 / p_{end}^* + o(1 / p_{end}^*)$
\end{theorem}

To prove Theorem~\ref{runtime_th} we need some extra facts.

\begin{lemma}
  $p_0^k = \Theta(p_{end}^*)$, if we consider $k$ as a constant.
\end{lemma}
\begin{proof}
  We can say that $p_0^k < p_{end}^*,$ as $p_{end}^*$ is a convex combination of $p_0^k$ and other values that are greater than $p_0^k$.

  Recall that $p_i^k = \left(\frac{\lambda}{n}\right)^{k - i} e^{-\lambda} (1 + o(1))$. Consider $p_{end}^*:$

  \begin{align}\label{p_end_1}
  \begin{split}
    p_{end}^* &= \sum\limits_{i = 0}^{k - 1} \pi_i^* p_i^k = \sum\limits_{i = 0}^{k - 1} C (1 - p_i^k) \binom{n}{k - i} p_i^k \\
    &= C \sum\limits_{i = 0}^{k - 1} \binom{n}{k - i} \left(\frac{\lambda}{n}\right)^{k - i} e^{-\lambda} (1 + o(1)) \\
    &\le C e^{-\lambda} \sum\limits_{i = 0}^{k - 1} \frac{\lambda^{k - i}}{(k - i)!} (1 + o(1))
  \end{split}
  \end{align}

  Now consider $1/C$:

  \begin{align}\label{const_in_p}
  \begin{split}
    1/C &= \sum{i = 0}^{k - 1} (1 - p_i^k) \binom{n}{k - i} \ge \binom{n}{k} (1 + o(1)) \\
    &\ge \frac{(n - k)^k}{k!} (1 + o(1)) = \frac{n^k}{k!} (1 + o(1))
  \end{split}
  \end{align}

  Putting Eq.~\ref{const_in_p} to Eq.~\ref{p_end_1}, we get:

  \begin{align}\label{p_end_2}
  \begin{split}
    p_{end}^* & \le \left(\frac{\lambda}{n}\right)^k  e^{-\lambda} \sum\limits_{i = 0}^{k - 1} \lambda^{-i} \frac{k!}{(k - i)!} (1 + o(1)) \\
    &= p_0^k \sum\limits_{i = 0}^{k - 1} \lambda^{-i} \frac{k!}{(k - i)!} (1 + o(1))
  \end{split}
  \end{align}

  If we consider $\lambda$ and $k$ as constants that does not depend on $n$, then Eq.~\ref{p_end_2} together with the fact that $p_0^k < p_{end}^*$ prove the lemma.
\end{proof}

Now we are ready to prove Theorem~\ref{runtime_th}.

\begin{proof}{Theorem~\ref{runtime_th}}

The expected runtime of the algorithm can be written as a series:

\begin{align*}
  T = \sum\limits_{t = 1}^{+\infty} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)).
\end{align*}

We define $p_{end}(0) = 0$ to simplify some equations.

Let us introduce some integer value $\delta$ such that $1 \le \delta$ and $\delta = o(1/p_{end}^*)$. The exact value of $\delta$ will be defined later in this theorem. Let us define the following values:
\begin{align*}
  T_1 = \sum\limits_{t = 1}^{\delta} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)), \\
  T_2 = \sum\limits_{t = \delta + 1}^{+\infty} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)).
\end{align*}

Our aim is to show that $T_1 = o(1/p_{end}^*)$ and $T_2 = 1/p_{end}^* + o(1/p_{end}^*).$ This two facts together will prove the theorem, as $T = T_1 + T_2$.

Consider $T_1$:

\begin{align*}
  T_1 &=  \sum\limits_{t = 1}^{\delta} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)) \\
  &\le \max\limits_{t \in [1, \delta]}(p_{end}(t)) \sum\limits_{t = 1}^{\delta} t (1 - p_{end}(1))^{t - 1} \\
  &\le \max\limits_{t \in [1, \delta]}(p_{end}(t))  \frac{1 - (1 - p_0^k)^\delta(1 + \delta p_0^k)}{(p_0^k)^2}.
\end{align*}

As we assumed, $|p_{end}(t) - p_{end}^*|$ is a decreasing sequence and $p_{end}(1) = p_0^k = \Theta(p_{end}^*)$ , then $\max\limits_{t \in [1, \delta]}(p_{end}(t)) = O(p_0^k)$. Then $\frac{\max\limits_{t \in [1, \delta]}(p_{end}(t))}{(p_0^k)^2} = O(\frac{1}{p_0^k})$.
Also as $\delta = o(\frac{1}{p_0^k})$, then $\lim\limits_{n \to + \infty} (1 - p_0^k)^\delta = 1$ and $\lim\limits_{n \to + \infty} (1 + \delta p_0^k) = 1$. So it gives us:

\begin{align*}
  T_1 = o(1) \cdot O\left(\frac{1}{p_0^k}\right) = o\left(\frac{1}{p_0^k}\right).
\end{align*}

Now consider $T_2$:

\begin{align*}
  T_2 &= \sum\limits_{t = \delta + 1}^{+\infty} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)) \\
      &= \prod\limits_{s = 0}^{\delta - 1} (1 - p_{end}(s)) \sum\limits_{t = \delta + 1}^{+\infty} t p_{end}(t) \prod\limits_{s = \delta + 1}^{t - 1} (1 - p_{end}(s))
\end{align*}

First, consider the factor we put out of the sum:
\begin{align*}
  1 > \prod\limits_{s = 0}^{\delta - 1} (1 - p_{end}(s)) > (1 - p_0^k)^\delta \xrightarrow[n \to +\infty]{} 1.
\end{align*}

So we can consider this factor as $(1 + o(1)).$

As $\lim\limits_{t \to +\infty} p_{end}(t) = p_{end}^*$, then $\exists \varepsilon > 0$ $\forall t > \delta: |p_{end}(t) - p_{end}^*| < \varepsilon$. So we can say that $\forall t > \delta$ $p_{end}^* - \varepsilon < p_{end}(t) < p_{end}^* + \varepsilon.$

Here we need $\varepsilon$ to be $o(p_{end}^*).$ If we assume that $|p_{end}(t) - p_{end}^*|$ is bounded above by a geometric progression with a common ratio equal to some constant $q < 1$, then we can choose $\delta = \sqrt{\frac{1}{p_{end}^*}}$ and it will give us $\varepsilon < q^{\sqrt{1/p_{end}^*}}= o(p_{end}^*).$

Introducing $\varepsilon$ gives us the following bounds for $T_2:$

\begin{align*}
T_2 &\ge (1 + O(1)) \sum\limits_{t = \delta + 1}^{+\infty} t (p_{end}^* - \varepsilon) (1 - p_{end}^* - \varepsilon)^{(t - \delta)} \\
T_2 &\le (1 + O(1)) \sum\limits_{t = \delta + 1}^{+\infty} t (p_{end}^* + \varepsilon) (1 - p_{end}^* + \varepsilon)^{(t - \delta)}
\end{align*}

And as long as we can consider $\varepsilon = o(p_{end}^*),$ then both bounds are asymptotically equal. Here we consider only the upper bound, as the lower one can be considered in a similar way.

\begin{align*}
  T_2 &\le (1 + O(1)) \sum\limits_{t = \delta + 1}^{+\infty} t (p_{end}^* + \varepsilon) (1 - p_{end}^* + \varepsilon)^{(t - \delta)} \\
  &= (1 + O(1)) \sum\limits_{t = 1}^{+\infty} (t + \delta) (p_{end}^* + \varepsilon) (1 - p_{end}^* + \varepsilon)^t \\
  &= (1 + o(1)) (p_{end}^* + \varepsilon) \left(\sum\limits_{t = 1}^{+\infty} t (1 - p_{end}^* + \varepsilon)^t +  \sum\limits_{t = 1}^{+\infty} \delta (1 - p_{end}^* + \varepsilon)^t \right) \\
  &= (1 + o(1)) (p_{end}^* + \varepsilon) \left( \frac{1 - p_{end}^* + \varepsilon}{(p_{end}^* - \varepsilon)^2} + \frac{\delta}{p_{end}^* - \varepsilon}\right) \\
  &= (1 + o(1)) p_{end}^* \left( \frac{1}{p_{end}^{*2}} + o\left(\frac{1}{p_{end}^{*2}}\right)\right) \\
  &= (1 + o(1))  \frac{1}{p_{end}^*}.
\end{align*}

Summing up:

\begin{align*}
T = T_1 + T_2 = \frac{1}{p_{end}^*} + o\left(\frac{1}{p_{end}^*}\right).
\end{align*}

\end{proof}

\section{Optimal value of $\lambda$}

The optimal value of $\lambda$ must maximize $p_{end}^*$. Consider $p_{end}^*:$

\begin{align*}
p_{end}^* &= C\sum\limits_{i = 0}^{k - 1} (1 - p_i^k) \binom{n}{k - i} \left( \frac{\lambda}{n} \right)^{k - i}  \left(1 - \frac{\lambda}{n} \right)^{n - k + i} \\
&= (1 + o(1))Ce^{-\lambda}\sum\limits_{i = 0}^{k - 1} \binom{n}{k - i} \left( \frac{\lambda}{n} \right)^{k - i}.
\end{align*}

It is easy to see that for $\lambda = 0$ and for $\lambda \to +\infty$ $p_{end}^*(\lambda) = 0$. So as long as $p_{end}^*(\lambda)$ is a continious function, we aim to find zeros of its derivative. As we need to find \textit{asyptotically optimal} value for $\lambda$, then it will be easier to consider approximate value of $p_{end}^*:$

\begin{align*}
 p_{end}^*  \approx p_a &= Ce^{-\lambda}\sum\limits_{i = 0}^{k - 1} \binom{n}{k - i} \left( \frac{\lambda}{n} \right)^{k - i} \\
                        &= Ce^{-\lambda}\sum\limits_{i = 1}^{k} \binom{n}{i} \left( \frac{\lambda}{n} \right)^{i}
\end{align*}

Its derivative:

\begin{align*}
(p_a)' &= Ce^{-\lambda}(\sum\limits_{i = 1}^{k} \binom{n}{i} \frac{i\lambda^{i - 1}}{n^i} - \sum\limits_{i = 1}^{k} \binom{n}{i} \left( \frac{\lambda}{n} \right)^{i}) \\
       &= Ce^{-\lambda}(\sum\limits_{i = 0}^{k - 1} \binom{n}{i + 1} \frac{(i + 1)\lambda^i}{n^{i + 1}} - \sum\limits_{i = 1}^{k} \binom{n}{i} \left( \frac{\lambda}{n} \right)^{i}) \\
       &= Ce^{-\lambda}\left(1 - \sum\limits_{i = 1}^{k - 1} \left(\binom{n}{i + 1} \frac{(i + 1)\lambda^{i}}{n^{i + 1}} - \binom{n}{i} \frac{\lambda^i}{n^i} \right) - \binom{n}{k} \frac{\lambda^k}{n^k} \right) \\
       &= Ce^{-\lambda}\left(1 - \sum\limits_{i = 1}^{k - 1} \frac{\lambda^i}{n^i} \left(\binom{n}{i} \frac{i + 1}{n} - \binom{n}{i} \right) - \binom{n}{k} \frac{\lambda^k}{n^k} \right) \\
       &= Ce^{-\lambda}\left(1 - \sum\limits_{i = 1}^{k - 1} \frac{\lambda^i}{n^i} \binom{n - 1}{i - 1} - \binom{n}{k} \frac{\lambda^k}{n^k} \right) \\
\end{align*}

$(p_a)' = 0$ only if $\left(1 - \sum\limits_{i = 1}^{k - 1} \frac{\lambda^i}{n^i} \binom{n - 1}{i - 1} - \binom{n}{k} \frac{\lambda^k}{n^k} \right) = 0$. Let us consider the last equation more precicely.

For all $i \in [1; k - 1]:$ 
\begin{align*}
  \binom{n - 1}{i - 1}\frac{\lambda^i}{n^i} = \frac{\lambda^i}{(i - 1)!} \frac{(n - 1)\cdot(n - 2)\cdots(n - i + 1)}{n^i} = \frac{\lambda^i}{(i - 1)!} (1/n + o(1/n))
\end{align*}

Thus, $\sum\limits_{i = 1}^{k - 1} \frac{\lambda^i}{n^i} \binom{n - 1}{i - 1} = o(1).$

We can say in the same way that $\binom{n}{k} \frac{\lambda^k}{n^k} = \frac{\lambda^k}{k!} (1 + o(1))$. So we have a simplified derivative of $p_a:$

\begin{align*}
(p_a)' = 1 - \frac{\lambda^k}{k!} + o(1).
\end{align*}

It means that if $\lambda \ne \sqrt[k]{k!}$, then for some large $n$ there is a neighbourhood of this value that does not contain zero of the derivative. At the same time if $\lambda = \sqrt[k]{k!}$, then $\forall \varepsilon > 0$ $\exists N$ $\forall n > N$ $|(p_a)'| < \varepsilon$. So, the only value of $\lambda$ that can be considered as asymptotically optimal is $\lambda = \sqrt[k]{k!}$.

\printbibliography

\end{document}
