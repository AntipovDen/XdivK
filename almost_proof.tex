\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{flushend}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{url}
\usepackage[table]{xcolor}
\usepackage{xspace}
\usepackage[T2A]{fontenc}

\usepackage{algorithm}
\usepackage[english]{babel}

\usepackage[utf8]{inputenc}
\usepackage[backend=bibtex]{biblatex}

\newcommand{\OM}{\textsc{OneMax}\xspace}
 \newcommand{\J}{\textsc{Jump}\xspace}
\newcommand{\LB}{\textsc{LeftBridge}\xspace}
\newcommand{\RB}{\textsc{RightBridge}\xspace}
\newcommand{\EARL}{\textsc{EA+RL}\xspace}
\newcommand{\RLS}{\textsc{RLS}\xspace}
\newcommand{\OMZM}{\textsc{OneMax+ZeroMax}\xspace}
\newcommand{\XdK}{\textsc{XdivK}\xspace}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\addbibresource{bibliography.bib}
\allowdisplaybreaks

\begin{document}

In this document I present a proof (without minor details) of the optimal value of the mutation rate for the $(1 + 1)$-ES optimizing a function with a plateau around its global optima of radius $k$. First three sections are identical to the first three sections of the previous paper ("current progress"), but with fixed bugs in math and in my English.

\section{Problem statement}

We consider simple $(1 + 1)$-ES. It has only one individual in population and on each iteration it creates a new individual by mutating each bit of the current individual with probability of $\frac{\lambda}{n}$, where $\lambda$ is a mutation rate that is a fixed constant during an algorithm run. It accepts the new individual if and only if its fitness value is not worse then the fitness value of the curent individual.

At first we considered \XdK function. But it seemed too complicated, so we simplified it to the following function $F$ that has some parameter $k$, as \XdK does:

\begin{align*}
  F(x) =
  \begin{cases}
    \OM(x), \text{ if } \OM(x) \le n - k \\
    n - k, \text{ if } n - k < \OM(x) < n \\
    n, \text{ if } \OM(x) = n \\
  \end{cases}
\end{align*}

Our aim is to find the optimal value of the mutation rate $\lambda$ that minimizes the expected runtime of the algorithm. If we reach our goal, we will be able to compare the considered algorithm with the Fast Genetic Algorithm.

\section{First subproblem}
As passing over the plateau in the end of optimization process requires most time (at least $\Omega(n^2)$ for $k \ge 2$, while reaching the plateau requires only $O(n \log n)$), we are most interested in finding the optimal value of $\lambda$ for passing the plateau. So first subproblem (that is actually our source problem) is to find a dependence of exact expected runtime of passing the plateau on the mutation rate.

\section{Notation}

We consider a markov chain that illustrates the behaviour of the considered algorithm on the plateau of the function $F$. It has $k + 1$ states that are numbered from $0$ to $k$. If the algorithm is in state $i$, then the current individual has exactly $n - k + i$ one-bits in it. State $k$ is obviously a final state, so we aim to find the expected number of iterations needed to get there.

By $p_i^j$ we imply the probability that algorithm being in state $i$ will get to state $j$ in one iteration.

The exact value of this probabilities is quite complicated:

\begin{align*}
  p_i^j = \begin{cases}
    \sum\limits_{m = 0}^{k - j} \binom{k - i}{j - i + m} \binom{n - k + i}{m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m}, \text{ if } j > i \\
      \sum\limits_{m = 0}^{k - i} \binom{k - i}{m} \binom{n - k + i}{i - j + m} \left(\frac{\lambda}{n}\right)^{i - j + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - i + j - 2m}, \text{ if } j < i \\
      1 - \sum\limits_{m = 0, m \ne i}^{k + 1} p_i^m, \text{ if } j = i \\
  \end{cases}
\end{align*}

Let us also notice that the first summand in the sum for $i \ne j$ is the most significant one: every next summand is less then previos one by factor at least $\frac{n}{k\lambda^2}$, so in some equations of this work we consider $p_i^j$ as its first summand multiplied by factor $(1 + o(1)).$

By $\pi_i(t)$ we imply the probability that algorithm will be in state $i$ on the $t$-th iteration with condition that algorithm does not reach the final state before $t$-th iteration. Also we define $\pi(t)$ as a stochastic vector $(\pi_0(t), \pi_2(t), \dots, \pi_{k - 1}(t)).$ Notice that $\pi(0) = (1, 0, \cdots, 0),$ although it does not affect the proof.

By $p_{end}(t)$ we imply the probability that algorithm will reach the final state on the $t$-th iteration with condition that it does not reach it before. Notice that:

$$p_{end}(t) = \sum\limits_{i = 0}^k \pi_i(t) p_i^k$$

\section{Plan of the proof}

First we will find a stationary distribution over the states of the markov chain, excluding state $k - 1$. It is a distribution $\pi^*$ such that

$$\forall i \in [0, k - 1] (\pi_i(t) = \pi^*_i \Rightarrow \pi_i(t + 1) = \pi^*_i).$$

Also we will find $p_{end}^*$ that is a probability to reach the final state for stationary distribution (just like $p_{end}(t)$ for $\pi(t)$).

Next step will be showing that $\pi(t)$ becomes very close to $\pi^*$ in just a few iterations. Then we will show that the total expected runtime is actually $\frac{1}{p_{end}^*} +o\left(\frac{1}{p_{end}^*} \right)$. The last step is finding the value of mutation rate $\lambda$ that minimizes $\frac{1}{p_{end}^*}.$

\section{Stationary distribution}

Obviously, algorithm will almost surely reach the final state. So the stationary distribution over all the states of markov chain will be $(0, \dots, 0, 1)$ and it does not help us to find the expected runtime. But we actualy are interested, what will be the distribution over the states of markov chain on the $t$-th iteration, if algorithm does not reach final state before this iteration.

From now we will consider slightly another markov chain. This chain has the same states as the original one, except the final state ($k$ states in total). It also has the same transition probabilities, but all the transition probabilities from state $i$ are normalized by factor $\frac{1}{1 - p_i^k},$ as we exclude the probability to go to the final state $k$. Transition matrix for this chain is $P = \left(\frac{p_i^j}{1 - p_i^k}\right).$ If we know $\pi(t)$ then we can find $\pi(t + 1) = \pi(t) \cdot P.$

In this section we aim to find a stationary distribution over new markov chain $\pi^*$ such that $\pi^* = \pi^* \cdot P.$


\begin{lemma}[Stationary distribution]
$\pi^* = C \left((1 - p_i^k) \binom{n}{k - i}\right)_{i = 0}^{k - 1},$ where $C = \left(\sum\limits_{i = 0}^{k - 1} (1 - p_i^k) \binom{n}{k - i}\right)^{-1}$ is normalization constant.
\end{lemma}

To prove this lemma we first need to prove an additional statement.

\begin{lemma}[There and back again]
$p_i^j \binom{n}{k - i} = p_j^i \binom{n}{k - j}$
\end{lemma}

\begin{proof}
For the case when $i = j$ this lemma is obvious.

Consider the case when $j > i.$ Then:

\begin{align*}
  p_i^j \binom{n}{k - i} &= \sum\limits_{m = 0}^{k - j} \binom{n}{k - i} \binom{k - i}{j - i + m} \binom{n - k + i}{m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m} \\
  &= \sum\limits_{m = 0}^{k - j} \binom{n}{k - j} \binom{k - j}{m} \binom{n - k + j}{j - i + m} \left(\frac{\lambda}{n}\right)^{j - i + 2m} \left(1 - \frac{\lambda}{n}\right)^{n - j + i - 2m} \\
  &= p_j^i \binom{n}{k - j}.
\end{align*}

The case when $j < i$ can be proven by switching $i$ and $j$ in the previous case.
\end{proof}

\begin{proof}[Stationary distribution]
Let $\pi$ be $\left(C (1 - p_i^k) \binom{n}{k - i}\right)_{i = 0}^{k - 1}$. We need to proof that $\pi = \pi \cdot P.$

For some integer $i \in [0, k - 1]$ consider the $i$-th element of $\pi \cdot P$:
\begin{align*}
(\pi \cdot P)_i &= \sum\limits_{j = 0}^{k - 1} \pi_j \frac{p_j^i}{1 - p_j^k} = \sum\limits_{j = 0}^{k - 1} C \binom{n}{k - j} p_j^i \\
&= \sum\limits_{j = 0}^{k - 1} C \binom{n}{k - i} p_i^j = C (1 - p_i^k) \binom{n}{k - i} \sum\limits_{j = 0}^{k - 1} \frac{p_i^j}{1 - p_i^k} \\
&= C (1 - p_i^k) \binom{n}{k - i} = \pi_i.
\end{align*}

It means that $\pi$ is a stable distribution for the markov chain, and as all the sates of the considered markov chain are irreducible and positive reccurent (\textit{is it obvious?}), then such distribution is unique, so $\pi^* = \pi$.
\end{proof}

\section{Convergence to the stable distribution}

This section has some spaces in it, but I do not think it will be hard to fill them.

As our chain is positive reccurent, irreducible and aperiodic (\textit{is it also obvious?}), then all $\pi_i(t) \to \pi^*_i$ if $t \to +\infty.$ The question we'd like to answer is how fast does it converge. Experiments has shown that for every $i$ the difference between $\pi_i(t)$ and $\pi^*_i$ absolutely decreases like a geometric progression with ration about $0.65$.

But actually we are interested in the speed of convergence of $p_{end}(t)$ to $p_{end}^*$. Let's consider their difference:

\begin{align*}
  |p_{end}^* - p_{end}(t)| &= |\sum\limits_{i = 0}^{k - 1} p_i^k \pi^*_i - \sum\limits_{i = 0}^{k - 1} p_i^k \pi_i(t)| = |\sum\limits_{i = 0}^{k - 1} p_i^k (\pi^*_i - \pi_i(t))| \\
  &\le \sum\limits_{i = 0}^{k - 1} (p_i^j)^2 \cdot |\pi^* - \pi(t)| \le |\pi^* - \pi(t)|.
\end{align*}

In the last inequation we used HÃ¶lder's inequality (for euclidean measure).

Now we will bound $|\pi^* - \pi(t)|.$ Let $e_1, e_2, \dots, e_k$ be left eigenvectors of the matrix $P$ and $|\lambda_1| \ge |\lambda_2| \ge \dots \ge |\lambda_k|$ be corresponding eigenvalues. As $P$ is a stochastic matrix, then $\lambda_1 = 1.$ $P$'s eigenvectors form a basis, so there are some constants $c_1, c_2, \dots, c_k$ such that $\pi(0) = \sum\limits_{i = 0}^k c_i e_i.$
Then $\pi(1) = \sum\limits_{i = 1}^k \lambda_i c_i e_i.$ After $t$ iterations $\pi(t) = \sum\limits_{i = 1}^k \lambda_i^t c_i e_i.$ Recall that $\lambda_1 = 1$ and $\pi^* = c_1 e_1$ then:

\begin{align*}
  |\pi^* - \pi(t)| &= |c_1 e_1 - \sum\limits_{i = 1}^k \lambda_i^t c_i e_i| = |\sum\limits_{i = 2}^k \lambda_i^t c_i e_i | \\
  &\le |\lambda_2|^t \sum\limits_{i = 2}^k |c_i e_i| = |\lambda_2|^t C_0,
\end{align*}

where $C_0$ is some constant. \textit{One of the spaces in this section is that I have not prooved that $C_0$ does not depent on $n$ (or at least it remains a constant for $n \to +\infty$).}

Now we can say that $|p_{end}^* - p_{end}(t)| \le C_0 |\lambda_2|^t$

Next space is proving that $\lambda_2 < const(n) < 1.$ It is the next space in this section, however the main idea of proving it is based on the Theorem 1 from \cite{secondev}. As experiments has shown, the value of $\lambda_2$ does not depend on $n$ but slightly depend on $k$ and is always less then $0.7$.

If we fill the spaces, then we will be able to prove that  $|p_{end}^* - p_{end}(t)|$ is geometric progression.

\section{Runtime proof}.

\begin{theorem}\label{runtime_th}
  The expected runtime of $(1 + 1)$-ES on the function $F$ is $p_{end}^{*-1} + o(p_{end}^{*-1})$
\end{theorem}

To prove Theorem~\ref{runtime_th} we need some extra facts.

\begin{lemma}
  $p_0^k = \Theta(p_{end}^*)$, if we consider $k$ as a constant.
\end{lemma}
\begin{proof}
  We can say that $p_0^k < p_{end}^*,$ as $p_{end}^*$ is a convex combination of $p_0^k$ and other values that are greater than $p_0^k$.

  Recall that $p_i^k = \left(\frac{\lambda}{n}\right)^{k - i} e^{-\lambda} (1 + o(1))$. Consider $p_{end}^*:$

  \begin{align}\label{p_end_1}
    p_{end}^* &= \sum\limits_{i = 0}^{k - 1} \pi_i^* p_i^k = \sum\limits_{i = 0}^{k - 1} C (1 - p_i^k) \binom{n}{k - i} p_i^k \\
    &= C \sum\limits_{i = 0}^{k - 1} \binom{n}{k - i} \left(\frac{\lambda}{n}\right)^{k - i} e^{-\lambda} (1 + o(1)) \\
    &\le C e^{-\lambda} \sum\limits_{i = 0}^{k - 1} \frac{\lambda^{k - i}}{(k - i)!} (1 + o(1))
  \end{align}

  Now consider $1/C$:

  \begin{align}\label{const_in_p}
    1/C &= \sum{i = 0}^{k - 1} (1 - p_i^k) \binom{n}{k - i} \ge \binom{n}{k} (1 + o(1)) \\
    &\ge \frac{(n - k)^k}{k!} (1 + o(1)) = \frac{n^k}{k!} (1 + o(1))
  \end{align}

  Putting Eq.~\ref{const_in_p} to Eq.~\ref{p_end_1}, we get:

  \begin{align*}\label{p_end_2}
    p_{end}^* & \le \left(\frac{\lambda}{n}\right)^k  e^{-\lambda} \sum\limits_{i = 0}^{k - 1} \lambda^{-i} \frac{k!}{(k - i)!} (1 + o(1)) \\
    &= p_0^k \sum\limits_{i = 0}^{k - 1} \lambda^{-i} \frac{k!}{(k - i)!} (1 + o(1))
  \end{align*}

  If we consider $\lambda$ and $k$ as constants that does not depend on $n$, then Eq.2 together with the fact that $p_0^k < p_}end^*$ proves the lemma.
\end{proof}

Now we are ready to prove Theorem~\ref{runtime_th}.

\begin{proof}{Theorem~\ref{runtime_th}}

The expected runtime of the algorithm can be written as a series:

\begin{align*}
  T = \sum\limits_{t = 1}^{+\infty} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)).
\end{align*}

We define $p_{end}(0) = 0$ to simplify some equations.

Let us introduce some integer value $\delta$ such that $1 \le \delta$ and $\delta = o(1/p_{end}^*)$. The exact value of $\delta$ will be defined later in this theorem. Let us define the following values:
\begin{align*}
  T_1 = \sum\limits_{t = 1}^{\delta} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)), \\
  T_2 = \sum\limits_{t = \delta + 1}^{+\infty} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s)).
\end{align*}

Our aim is to show that $T_1 = o(1/p_{end}^*)$ and $T_2 = 1/p_{end}^* + o(1/p_{end}^*).$ This two facts will prove the theorem, as $T = T_1 + T_2$.

Consider $T_1$:

\begin{align*}
  T_1 =  \sum\limits_{t = 1}^{\delta} t p_{end}(t) \prod\limits_{s = 0}^{t - 1} (1 - p_{end}(s))
\end{align*}
\end{proof}

\printbibliography

\end{document}
